{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Search for Beginners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "If you are new to vector databases, this tutorial is for you. In 5 minutes you will build a semantic search engine for science fiction books. After you set it up, you will ask the engine about an impending alien threat. Your creation will recommend books as preparation for a potential space attack.\n",
    "\n",
    "Before you begin, you need to have a recent version of Python installed. If you don’t know how to run this code in a virtual environment, follow Python documentation for Creating Virtual Environments first.\n",
    "\n",
    "This tutorial assumes you’re in the bash shell. Use the Python documentation to activate a virtual environment, with commands such as:\n",
    "\n",
    "\n",
    "```bash \n",
    "source tutorial-env/bin/activate\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting qdrant-client\n",
      "  Using cached qdrant_client-1.10.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting transformers<5.0.0,>=4.34.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.42.3-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm (from sentence-transformers)\n",
      "  Using cached tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Using cached torch-2.3.1-cp312-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
      "Collecting numpy (from sentence-transformers)\n",
      "  Using cached numpy-2.0.0-cp312-cp312-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.5.1-cp312-cp312-macosx_12_0_arm64.whl.metadata (12 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Using cached scipy-1.14.0-cp312-cp312-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Collecting huggingface-hub>=0.15.1 (from sentence-transformers)\n",
      "  Using cached huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting Pillow (from sentence-transformers)\n",
      "  Downloading pillow-10.4.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Collecting grpcio>=1.41.0 (from qdrant-client)\n",
      "  Using cached grpcio-1.64.1-cp312-cp312-macosx_10_9_universal2.whl.metadata (3.3 kB)\n",
      "Collecting grpcio-tools>=1.41.0 (from qdrant-client)\n",
      "  Downloading grpcio_tools-1.64.1-cp312-cp312-macosx_10_9_universal2.whl.metadata (5.3 kB)\n",
      "Collecting httpx>=0.20.0 (from httpx[http2]>=0.20.0->qdrant-client)\n",
      "  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting portalocker<3.0.0,>=2.7.0 (from qdrant-client)\n",
      "  Using cached portalocker-2.10.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting pydantic>=1.10.8 (from qdrant-client)\n",
      "  Downloading pydantic-2.8.2-py3-none-any.whl.metadata (125 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting urllib3<3,>=1.26.14 (from qdrant-client)\n",
      "  Using cached urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting protobuf<6.0dev,>=5.26.1 (from grpcio-tools>=1.41.0->qdrant-client)\n",
      "  Using cached protobuf-5.27.2-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Collecting setuptools (from grpcio-tools>=1.41.0->qdrant-client)\n",
      "  Using cached setuptools-70.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting anyio (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client)\n",
      "  Using cached anyio-4.4.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting certifi (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client)\n",
      "  Downloading certifi-2024.7.4-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting httpcore==1.* (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client)\n",
      "  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting idna (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client)\n",
      "  Using cached idna-3.7-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting sniffio (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting h2<5,>=3 (from httpx[http2]>=0.20.0->qdrant-client)\n",
      "  Using cached h2-4.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting filelock (from huggingface-hub>=0.15.1->sentence-transformers)\n",
      "  Using cached filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.15.1->sentence-transformers)\n",
      "  Using cached fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/jeongmin/PycharmProjects/qdrant-in-practice/myenv/lib/python3.12/site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.1)\n",
      "Collecting pyyaml>=5.1 (from huggingface-hub>=0.15.1->sentence-transformers)\n",
      "  Using cached PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting requests (from huggingface-hub>=0.15.1->sentence-transformers)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub>=0.15.1->sentence-transformers)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic>=1.10.8->qdrant-client)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.20.1 (from pydantic>=1.10.8->qdrant-client)\n",
      "  Downloading pydantic_core-2.20.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting sympy (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading sympy-1.13.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting jinja2 (from torch>=1.11.0->sentence-transformers)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting numpy (from sentence-transformers)\n",
      "  Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers<5.0.0,>=4.34.0->sentence-transformers)\n",
      "  Using cached regex-2024.5.15-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.34.0->sentence-transformers)\n",
      "  Using cached safetensors-0.4.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers<5.0.0,>=4.34.0->sentence-transformers)\n",
      "  Using cached tokenizers-0.19.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting hyperframe<7,>=6.0 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client)\n",
      "  Using cached hyperframe-6.0.1-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting hpack<5,>=4.0 (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client)\n",
      "  Using cached hpack-4.0.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch>=1.11.0->sentence-transformers)\n",
      "  Using cached MarkupSafe-2.1.5-cp312-cp312-macosx_10_9_universal2.whl.metadata (3.0 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->huggingface-hub>=0.15.1->sentence-transformers)\n",
      "  Using cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (33 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch>=1.11.0->sentence-transformers)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached qdrant_client-1.10.1-py3-none-any.whl (254 kB)\n",
      "Using cached grpcio-1.64.1-cp312-cp312-macosx_10_9_universal2.whl (10.3 MB)\n",
      "Downloading grpcio_tools-1.64.1-cp312-cp312-macosx_10_9_universal2.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hUsing cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Using cached huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "Using cached portalocker-2.10.0-py3-none-any.whl (18 kB)\n",
      "Downloading pydantic-2.8.2-py3-none-any.whl (423 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.9/423.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pydantic_core-2.20.1-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached torch-2.3.1-cp312-none-macosx_11_0_arm64.whl (61.0 MB)\n",
      "Using cached tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "Downloading transformers-4.42.3-py3-none-any.whl (9.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\n",
      "Using cached urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "Downloading pillow-10.4.0-cp312-cp312-macosx_11_0_arm64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.5.1-cp312-cp312-macosx_12_0_arm64.whl (11.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached scipy-1.14.0-cp312-cp312-macosx_14_0_arm64.whl (23.1 MB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Using cached h2-4.1.0-py3-none-any.whl (57 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached protobuf-5.27.2-cp38-abi3-macosx_10_9_universal2.whl (412 kB)\n",
      "Using cached PyYAML-6.0.1-cp312-cp312-macosx_11_0_arm64.whl (165 kB)\n",
      "Using cached regex-2024.5.15-cp312-cp312-macosx_11_0_arm64.whl (278 kB)\n",
      "Using cached safetensors-0.4.3-cp312-cp312-macosx_11_0_arm64.whl (411 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Using cached tokenizers-0.19.1-cp312-cp312-macosx_11_0_arm64.whl (2.4 MB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached anyio-4.4.0-py3-none-any.whl (86 kB)\n",
      "Using cached idna-3.7-py3-none-any.whl (66 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading certifi-2024.7.4-py3-none-any.whl (162 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.0/163.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached setuptools-70.2.0-py3-none-any.whl (930 kB)\n",
      "Downloading sympy-1.13.0-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached charset_normalizer-3.3.2-cp312-cp312-macosx_11_0_arm64.whl (119 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached hpack-4.0.0-py3-none-any.whl (32 kB)\n",
      "Using cached hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
      "Using cached MarkupSafe-2.1.5-cp312-cp312-macosx_10_9_universal2.whl (18 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, urllib3, typing-extensions, tqdm, threadpoolctl, sympy, sniffio, setuptools, safetensors, regex, pyyaml, protobuf, portalocker, Pillow, numpy, networkx, MarkupSafe, joblib, idna, hyperframe, hpack, h11, grpcio, fsspec, filelock, charset-normalizer, certifi, annotated-types, scipy, requests, pydantic-core, jinja2, httpcore, h2, grpcio-tools, anyio, torch, scikit-learn, pydantic, huggingface-hub, httpx, tokenizers, transformers, qdrant-client, sentence-transformers\n",
      "Successfully installed MarkupSafe-2.1.5 Pillow-10.4.0 annotated-types-0.7.0 anyio-4.4.0 certifi-2024.7.4 charset-normalizer-3.3.2 filelock-3.15.4 fsspec-2024.6.1 grpcio-1.64.1 grpcio-tools-1.64.1 h11-0.14.0 h2-4.1.0 hpack-4.0.0 httpcore-1.0.5 httpx-0.27.0 huggingface-hub-0.23.4 hyperframe-6.0.1 idna-3.7 jinja2-3.1.4 joblib-1.4.2 mpmath-1.3.0 networkx-3.3 numpy-1.26.4 portalocker-2.10.0 protobuf-5.27.2 pydantic-2.8.2 pydantic-core-2.20.1 pyyaml-6.0.1 qdrant-client-1.10.1 regex-2024.5.15 requests-2.32.3 safetensors-0.4.3 scikit-learn-1.5.1 scipy-1.14.0 sentence-transformers-3.0.1 setuptools-70.2.0 sniffio-1.3.1 sympy-1.13.0 threadpoolctl-3.5.0 tokenizers-0.19.1 torch-2.3.1 tqdm-4.66.4 transformers-4.42.3 typing-extensions-4.12.2 urllib3-2.2.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sentence-transformers qdrant-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the models\n",
    "Once the two main frameworks are defined, you need to specify the exact models this engine will use. Before you do, activate the Python prompt (>>>) with the python command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeongmin/PycharmProjects/qdrant-in-practice/myenv/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from qdrant_client import models, QdrantClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sentence Transformers framework contains many embedding models. However, all-MiniLM-L6-v2 is the fastest encoder for this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Add the dataset\n",
    "all-MiniLM-L6-v2 will encode the data you provide. Here you will list all the science fiction books in your library. Each book has metadata, a name, author, publication year and a short description.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    {\n",
    "        \"name\": \"The Time Machine\",\n",
    "        \"description\": \"A man travels through time and witnesses the evolution of humanity.\",\n",
    "        \"author\": \"H.G. Wells\",\n",
    "        \"year\": 1895,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Ender's Game\",\n",
    "        \"description\": \"A young boy is trained to become a military leader in a war against an alien race.\",\n",
    "        \"author\": \"Orson Scott Card\",\n",
    "        \"year\": 1985,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Brave New World\",\n",
    "        \"description\": \"A dystopian society where people are genetically engineered and conditioned to conform to a strict social hierarchy.\",\n",
    "        \"author\": \"Aldous Huxley\",\n",
    "        \"year\": 1932,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"The Hitchhiker's Guide to the Galaxy\",\n",
    "        \"description\": \"A comedic science fiction series following the misadventures of an unwitting human and his alien friend.\",\n",
    "        \"author\": \"Douglas Adams\",\n",
    "        \"year\": 1979,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Dune\",\n",
    "        \"description\": \"A desert planet is the site of political intrigue and power struggles.\",\n",
    "        \"author\": \"Frank Herbert\",\n",
    "        \"year\": 1965,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Foundation\",\n",
    "        \"description\": \"A mathematician develops a science to predict the future of humanity and works to save civilization from collapse.\",\n",
    "        \"author\": \"Isaac Asimov\",\n",
    "        \"year\": 1951,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Snow Crash\",\n",
    "        \"description\": \"A futuristic world where the internet has evolved into a virtual reality metaverse.\",\n",
    "        \"author\": \"Neal Stephenson\",\n",
    "        \"year\": 1992,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Neuromancer\",\n",
    "        \"description\": \"A hacker is hired to pull off a near-impossible hack and gets pulled into a web of intrigue.\",\n",
    "        \"author\": \"William Gibson\",\n",
    "        \"year\": 1984,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"The War of the Worlds\",\n",
    "        \"description\": \"A Martian invasion of Earth throws humanity into chaos.\",\n",
    "        \"author\": \"H.G. Wells\",\n",
    "        \"year\": 1898,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"The Hunger Games\",\n",
    "        \"description\": \"A dystopian society where teenagers are forced to fight to the death in a televised spectacle.\",\n",
    "        \"author\": \"Suzanne Collins\",\n",
    "        \"year\": 2008,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"The Andromeda Strain\",\n",
    "        \"description\": \"A deadly virus from outer space threatens to wipe out humanity.\",\n",
    "        \"author\": \"Michael Crichton\",\n",
    "        \"year\": 1969,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"The Left Hand of Darkness\",\n",
    "        \"description\": \"A human ambassador is sent to a planet where the inhabitants are genderless and can change gender at will.\",\n",
    "        \"author\": \"Ursula K. Le Guin\",\n",
    "        \"year\": 1969,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"The Three-Body Problem\",\n",
    "        \"description\": \"Humans encounter an alien civilization that lives in a dying system.\",\n",
    "        \"author\": \"Liu Cixin\",\n",
    "        \"year\": 2008,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define storage location\n",
    "You need to tell Qdrant where to store embeddings. This is a basic demo, so your local computer will use its memory as temporary storage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(\":memory:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a collection\n",
    "All data in Qdrant is organized by collections. In this case, you are storing books, so we are calling it my_books.\n",
    "\n",
    "Use recreate_collection if you are experimenting and running the script several times. This function will first try to remove an existing collection with the same name.\n",
    "\n",
    "The vector_size parameter defines the size of the vectors for a specific collection. If their size is different, it is impossible to calculate the distance between them. 384 is the encoder output dimensionality. You can also use model.get_sentence_embedding_dimension() to get the dimensionality of the model you are using.\n",
    "\n",
    "The distance parameter lets you specify the function used to measure the distance between two points.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r4/w6gk0qbd6bd_sf7xj6nwdnxc0000gn/T/ipykernel_95632/2679950266.py:1: DeprecationWarning: `recreate_collection` method is deprecated and will be removed in the future. Use `collection_exists` to check collection existence and `create_collection` instead.\n",
      "  client.recreate_collection(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.recreate_collection(\n",
    "    collection_name=\"my_books\",\n",
    "    vectors_config=models.VectorParams(\n",
    "        size=encoder.get_sentence_embedding_dimension(),  # Vector size is defined by used model\n",
    "        distance=models.Distance.COSINE,\n",
    "    ),\n",
    ") \n",
    "\n",
    "encoder.get_sentence_embedding_dimension()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Upload data to collection\n",
    "Tell the database to upload documents to the my_books collection. This will give each record an id and a payload. The payload is just the metadata from the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.upload_points(\n",
    "    collection_name=\"my_books\",\n",
    "    points=[\n",
    "        models.PointStruct(\n",
    "            id=idx, vector=encoder.encode(doc[\"description\"]).tolist(), payload=doc\n",
    "        )\n",
    "        for idx, doc in enumerate(documents)\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ask the engine a question\n",
    "Now that the data is stored in Qdrant, you can ask it questions and receive semantically relevant results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'The War of the Worlds', 'description': 'A Martian invasion of Earth throws humanity into chaos.', 'author': 'H.G. Wells', 'year': 1898} score: 0.5700932553268891\n",
      "{'name': \"The Hitchhiker's Guide to the Galaxy\", 'description': 'A comedic science fiction series following the misadventures of an unwitting human and his alien friend.', 'author': 'Douglas Adams', 'year': 1979} score: 0.5040467274998736\n",
      "{'name': 'The Three-Body Problem', 'description': 'Humans encounter an alien civilization that lives in a dying system.', 'author': 'Liu Cixin', 'year': 2008} score: 0.45902931148429993\n"
     ]
    }
   ],
   "source": [
    "hits = client.search(\n",
    "    collection_name=\"my_books\",\n",
    "    query_vector=encoder.encode(\"alien invasion\").tolist(),\n",
    "    limit=3,\n",
    ")\n",
    "for hit in hits:\n",
    "    print(hit.payload, \"score:\", hit.score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Narrow down the query\n",
    "\n",
    "How about the most recent book from the early 2000s?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'The Three-Body Problem', 'description': 'Humans encounter an alien civilization that lives in a dying system.', 'author': 'Liu Cixin', 'year': 2008} score: 0.45902931148429993\n"
     ]
    }
   ],
   "source": [
    "hits = client.search(\n",
    "    collection_name=\"my_books\",\n",
    "    query_vector=encoder.encode(\"alien invasion\").tolist(),\n",
    "    query_filter=models.Filter(\n",
    "        must=[models.FieldCondition(key=\"year\", range=models.Range(gte=2000))]\n",
    "    ),\n",
    "    limit=1,\n",
    ")\n",
    "for hit in hits:\n",
    "    print(hit.payload, \"score:\", hit.score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
